{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def data_from_json(filename):\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data\n",
    "\n",
    "train_data = data_from_json('datasets/train-v1.1.json')\n",
    "dev_data = data_from_json('datasets/dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check an instance from the dataset to see how it actually looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "qas:  [{'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}], 'id': '5733be284776f41900661182'}, {'question': 'What is in front of the Notre Dame Main Building?', 'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}], 'id': '5733be284776f4190066117f'}, {'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'answers': [{'answer_start': 279, 'text': 'the Main Building'}], 'id': '5733be284776f41900661180'}, {'question': 'What is the Grotto at Notre Dame?', 'answers': [{'answer_start': 381, 'text': 'a Marian place of prayer and reflection'}], 'id': '5733be284776f41900661181'}, {'question': 'What sits on top of the Main Building at Notre Dame?', 'answers': [{'answer_start': 92, 'text': 'a golden statue of the Virgin Mary'}], 'id': '5733be284776f4190066117e'}]\n"
     ]
    }
   ],
   "source": [
    "val = train_data['data'][0]\n",
    "para = val['paragraphs']\n",
    "print('context: ', para[0]['context'])\n",
    "print('qas: ', para[0]['qas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of samples present in training and dev data. Play around! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_paras: 18896 num_articles: 442\n",
      "train_data: 87599\n",
      "num_paras: 2067 num_articles: 48\n",
      "dev_data: 10570\n"
     ]
    }
   ],
   "source": [
    "def total_examples(data):\n",
    "    total_ques = 0\n",
    "    total_paras = 0\n",
    "    total_articles = 0\n",
    "    for article in data['data']:\n",
    "        total_articles += 1\n",
    "        total_paras += len(article['paragraphs'])\n",
    "        for para in article['paragraphs']:\n",
    "            total_ques += len(para['qas'])     \n",
    "    print('num_paras:', total_paras, 'num_articles:' , total_articles)\n",
    "    return total_ques\n",
    "\n",
    "print('train_data:', total_examples(train_data))\n",
    "print('dev_data:', total_examples(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['University_of_Notre_Dame', 'Beyoncé', 'Montana', 'Genocide', 'Antibiotics', 'Frédéric_Chopin', 'Sino-Tibetan_relations_during_the_Ming_dynasty', 'IPod', 'The_Legend_of_Zelda:_Twilight_Princess', 'Spectre_(2015_film)', '2008_Sichuan_earthquake', 'New_York_City', 'To_Kill_a_Mockingbird', 'Solar_energy', 'Tajikistan', 'Anthropology', 'Portugal', 'Kanye_West', 'Buddhism', 'American_Idol', 'Dog', '2008_Summer_Olympics_torch_relay', 'Alfred_North_Whitehead', 'Financial_crisis_of_2007%E2%80%9308', 'Saint_Barth%C3%A9lemy']\n"
     ]
    }
   ],
   "source": [
    "def list_topics(data):\n",
    "    list_topics = [data['data'][idx]['title'] for idx in range(0,len(data['data']))]\n",
    "    return list_topics\n",
    "\n",
    "print(list_topics(train_data)[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We reduce the size of the dataset so that we can train our models easily, even with less computational resources. This will also give us a lot of flexibilty to try more models, faster but obviously, at the cost of performance!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694 12723 12723 12723\n",
      "0\n",
      "888 4913 4913 4913\n"
     ]
    }
   ],
   "source": [
    "train_context = []\n",
    "train_questions = []\n",
    "train_answer = []\n",
    "train_answer_span = []\n",
    "\n",
    "dev_context = []\n",
    "dev_questions = []\n",
    "dev_answer = []\n",
    "dev_answer_span = []\n",
    "\n",
    "incorrect_ans = 0\n",
    "\n",
    "def segregate_lists(data, num, list_context, list_questions, list_answer, list_answer_span):\n",
    "    for article in data['data'][:num]:\n",
    "        for para in article['paragraphs']:\n",
    "            list_context.append(para['context'])\n",
    "            for qa in para['qas']:\n",
    "                list_questions.append(qa['question'])\n",
    "                ans_text = qa['answers'][0]['text']\n",
    "                list_answer.append(ans_text)\n",
    "                list_answer_span.append([qa['answers'][0]['answer_start'], qa['answers'][0]['answer_start']+len(ans_text)])\n",
    "                \n",
    "                #check if the answer spans actually correspond to the given answers \n",
    "                if ans_text != para['context'][qa['answers'][0]['answer_start']:qa['answers'][0]['answer_start']+len(ans_text)]:\n",
    "                    incorrect_ans +=1\n",
    "                    print(ans_text)\n",
    "                    print(para['context'][qa['answers'][0]['answer_start']:qa['answers'][0]['answer_start']+len(ans_text)])\n",
    "\n",
    "segregate_lists(train_data, 50, train_context, train_questions, train_answer, train_answer_span)\n",
    "print (len(train_context), len(train_questions), len(train_answer), len(train_answer_span))\n",
    "#print(train_answer_span)\n",
    "print(incorrect_ans)\n",
    "segregate_lists(dev_data, 20, dev_context, dev_questions, dev_answer, dev_answer_span)\n",
    "print (len(dev_context), len(dev_questions), len(dev_answer), len(dev_answer_span))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the longest context and question based on the no. of words. These values will later be used in preparing input data for the Embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of context:  629 words.\n",
      "max length of question:  29 words\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "context_maxlen_str= max(train_context+dev_context, key=len)\n",
    "ques_maxlen_str = max(train_questions+dev_questions, key =len)\n",
    "\n",
    "for c in string.punctuation:\n",
    "    context_maxlen_str= context_maxlen_str.replace(c,\"\")\n",
    "    ques_maxlen_str = ques_maxlen_str.replace(c,\"\")\n",
    "\n",
    "\n",
    "context_maxlen = len(context_maxlen_str.split())\n",
    "ques_maxlen = len(ques_maxlen_str.split())\n",
    "print('max length of context: ', context_maxlen, 'words.\\nmax length of question: ', ques_maxlen , 'words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use pre-trained word embeddings in the embedding layer of our model? \n",
    "1) Convert all text samples in the dataset into sequences of word indices. Word index = integer ID for the word.\n",
    "2) Construct an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "3) Load this embedding matrix into a Keras Embedding layer\n",
    "\n",
    "The Tokenizer class in Keras allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32061 unique tokens.\n",
      "(4913, 29)\n",
      "(888, 629)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts(train_context+dev_context+train_questions+dev_questions) # stores everything in the word_index\n",
    "\n",
    "train_context_sequences = tokenizer.texts_to_sequences(train_context) # Transforms each text in the argument in a sequence of integers.\n",
    "train_ques_sequences = tokenizer.texts_to_sequences(train_questions)\n",
    "dev_context_sequences= tokenizer.texts_to_sequences(dev_context)\n",
    "dev_ques_sequences = tokenizer.texts_to_sequences(dev_questions)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('%s unique tokens.' % len(word_index))\n",
    "\n",
    "#pad_sequences is used to ensure that all sequences in a list have the same length.\n",
    "train_context_data = pad_sequences(train_context_sequences, maxlen=context_maxlen)\n",
    "dev_context_data = pad_sequences(dev_context_sequences, maxlen= context_maxlen)\n",
    "\n",
    "train_ques_data = pad_sequences(train_ques_sequences, maxlen= ques_maxlen)\n",
    "dev_ques_data = pad_sequences(dev_ques_sequences, maxlen= ques_maxlen)\n",
    "\n",
    "print(dev_ques_data.shape)\n",
    "print(dev_context_data.shape)\n",
    "#print(tokenizer.word_index) - dictionary mapping words (str) to their rank/index (int)\n",
    "#print(sequences)\n",
    "# print(tokenizer.word_counts) - dictionary mapping words (str) to the number of times they appeared on during fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the embedding layer\n",
    "We compute an index, mapping words to known embeddings, by parsing the data dump of pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100 #dimension of gloVe \n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1,  EMBEDDING_DIM)) \n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load this embedding matrix into an Embedding layer. We set trainable=False to prevent the weights from being updated during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "ques_embedding_layer = Embedding(len(word_index) + 1, #input_dim: vocab_size \n",
    "                            EMBEDDING_DIM, # the size of the output vectors from this layer\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=ques_maxlen, # length of input sequences\n",
    "                            trainable=False)\n",
    "context_embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=context_maxlen,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Input(shape=(context_maxlen, EMBEDDING_DIM), name='P')\n",
    "Q = Input(shape=(ques_maxlen, EMBEDDING_DIM), name='Q')\n",
    "W = 28\n",
    "passage_input = P\n",
    "question_input = Q\n",
    "encoder = Bidirectional(LSTM(units=W,return_sequences=True))\n",
    "\n",
    "passage_encoding = P\n",
    "passage_encoding = encoder(passage_encoding)\n",
    "passage_encoding = TimeDistributed(Dense(W, use_bias=False, trainable=True, weights=np.concatenate((np.eye(W), np.eye(W)), axis=1)))(passage_encoding)\n",
    "\n",
    "question_encoding = Q\n",
    "question_encoding = encoder(question_encoding)\n",
    "question_encoding = TimeDistributed(Dense(W, use_bias=False, trainable=True, weights=np.concatenate((np.eye(W), np.eye(W)), axis=1)))(question_encoding)\n",
    "\n",
    "question_attention_vector = TimeDistributed(Dense(1))(question_encoding)\n",
    "question_attention_vector = Lambda(lambda q: keras.activations.softmax(q, axis=1))(question_attention_vector)\n",
    "\n",
    "question_attention_vector = Lambda(lambda q: q[0] * q[1])([question_encoding, question_attention_vector])\n",
    "question_attention_vector = Lambda(lambda q: K.sum(q, axis=1))(question_attention_vector)\n",
    "question_attention_vector = RepeatVector(context_maxlen)(question_attention_vector)\n",
    "\n",
    "answer_start = Lambda(lambda arg: concatenate([arg[0], arg[1], arg[2]]))([\n",
    "            passage_encoding,\n",
    "            question_attention_vector,\n",
    "            multiply([passage_encoding, question_attention_vector])])\n",
    "\n",
    "answer_start = TimeDistributed(Dense(W, activation='relu'))(answer_start)\n",
    "answer_start = TimeDistributed(Dense(1))(answer_start)\n",
    "answer_start = Flatten()(answer_start)\n",
    "answer_start = Activation('softmax')(answer_start)\n",
    "\n",
    "# Answer end prediction depends on the start prediction\n",
    "def s_answer_feature(x):\n",
    "    maxind = K.argmax( x,axis=1,)\n",
    "    return maxind\n",
    "\n",
    "x = Lambda(lambda x: K.tf.cast(s_answer_feature(x), dtype=K.tf.int32))(answer_start)\n",
    "start_feature = Lambda(lambda arg: K.tf.gather_nd(arg[0], K.tf.stack(\n",
    "            [K.tf.range(K.tf.shape(arg[1])[0]), K.tf.cast(arg[1], K.tf.int32)], axis=1)))([passage_encoding, x])\n",
    "start_feature = RepeatVector(context_maxlen)(start_feature)\n",
    "\n",
    "# Answer end prediction\n",
    "answer_end = Lambda(lambda arg: concatenate([\n",
    "            arg[0],\n",
    "            arg[1],\n",
    "            arg[2],\n",
    "            multiply([arg[0], arg[1]]),\n",
    "            multiply([arg[0], arg[2]])]))([passage_encoding, question_attention_vector, start_feature])\n",
    "\n",
    "answer_end = TimeDistributed(Dense(W, activation='relu'))(answer_end)\n",
    "answer_end = TimeDistributed(Dense(1))(answer_end)\n",
    "answer_end = Flatten()(answer_end)\n",
    "answer_end = Activation('softmax')(answer_end)\n",
    "\n",
    "input_placeholders = [P, Q]\n",
    "inputs = input_placeholders\n",
    "outputs = [answer_start, answer_end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
